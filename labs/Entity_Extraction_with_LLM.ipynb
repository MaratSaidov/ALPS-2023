{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8fc3ad",
   "metadata": {},
   "source": [
    "# Context\n",
    "Entity Extraction is a historical problem in NLP, and still one of the most useful one.\n",
    "It consists in extracting key entities out of a piece of text, where an entity is a grounded concept of interest.\n",
    "\n",
    "We are going to work with one called `WNUT 17` which contains _rare_ entities, extracted from user-generated content (comments, twitter). All the text we will be working with is in English.\n",
    "\n",
    "This is a challenging dataset, because of the rarity of the entities and the noisineses of the text. To get an idea of the state-of-the-art you can look at current results [here](https://paperswithcode.com/sota/named-entity-recognition-on-wnut-2017).\n",
    "\n",
    "Here we are going to try to solve this nail with a big ðŸ”¨: a **large language model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc34fcb",
   "metadata": {},
   "source": [
    "# Data\n",
    "We will use the `datasets` library from HuggingFace for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e073a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0df551",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('wnut_17')\n",
    "train = dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace28d5",
   "metadata": {},
   "source": [
    "A few helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map indexes to names of types of entities\n",
    "ner_names = list(range(15))\n",
    "ner_names[0] = 'Outside'\n",
    "ner_names[1] = 'B-corporation'\n",
    "ner_names[2] = 'I-corporation'\n",
    "ner_names[3] = 'B-creative-work'\n",
    "ner_names[4] = 'I-creative-work'\n",
    "ner_names[5] = 'B-group'\n",
    "ner_names[6] = 'I-group'\n",
    "ner_names[7] = 'B-location'\n",
    "ner_names[8] = 'I-location'\n",
    "ner_names[9] = 'B-person'\n",
    "ner_names[10] = 'I-person'\n",
    "ner_names[11] = 'B-location'\n",
    "ner_names[12] = 'B-product'\n",
    "ner_names[13] = 'B-location'\n",
    "ner_names[14] = 'I-product'\n",
    "\n",
    "class Entity:\n",
    "    \"\"\"\n",
    "        a Entity consists of two things: its type and the string describing it\n",
    "        taken from https://github.com/cohere-ai/cohere-python/blob/main/cohere/extract.py\n",
    "    \"\"\"\n",
    "    def __init__(self, type: str, value: str) -> None:\n",
    "        self.type = type\n",
    "        self.value = value\n",
    "\n",
    "    def toDict(self) -> dict:\n",
    "        return {\"type\": self.type, \"value\": self.value}\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"[{self.type}] {self.value}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self.toDict())\n",
    "\n",
    "    def __eq__(self, other) -> bool:\n",
    "        return self.type == other.type and self.value == other.value\n",
    "\n",
    "    \n",
    "def is_beginning(nt_idx):\n",
    "    return ner_names[nt_idx][0] == 'B'\n",
    "\n",
    "def _get_entities(d):\n",
    "    \"\"\"\n",
    "        returns entities as the concatenation of the contiguous strings\n",
    "    \"\"\"\n",
    "    cur_tag, cur_txt = None, None\n",
    "    for idx,nt in enumerate(d['ner_tags']):\n",
    "        if nt == 0: \n",
    "            continue\n",
    "        if is_beginning(nt):\n",
    "            if cur_txt is not None:\n",
    "                yield ' '.join(cur_txt), ner_names[cur_tag]\n",
    "            cur_txt = [ d['tokens'][idx ] ] # start new Named Entity\n",
    "            cur_tag = nt\n",
    "        else: # is Inside of the same tag\n",
    "            cur_txt.append( d['tokens'][idx])\n",
    "    # maybe there is a started NE\n",
    "    if cur_txt is not None:\n",
    "        yield ' '.join(cur_txt), ner_names[cur_tag]\n",
    "        \n",
    "def entities(d):\n",
    "    \"\"\"\n",
    "        returns list of Entity classes\n",
    "    \"\"\"\n",
    "    result = [] \n",
    "    for ents in _get_entities(d):\n",
    "        result.append(Entity(ents[1][2:], ents[0]))\n",
    "    return result\n",
    "\n",
    "\n",
    "def to_text(datapoint):\n",
    "    return ' '.join(datapoint['tokens'])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567cef1",
   "metadata": {},
   "source": [
    "A few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [2,31, 100, 42, 2021]:\n",
    "    print('TEXT:', to_text(train[idx]))\n",
    "    print('ENTITIES: ')\n",
    "    for e in list(entities(train[idx])):\n",
    "        print('\\t',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c7875",
   "metadata": {},
   "source": [
    "# Using Large Language Models (LLMs)\n",
    "\n",
    "Sign-up to cohere.com using this [link](https://dashboard.cohere.ai/welcome/register?utm_source=cohere-owned&utm_medium=event&utm_campaign=matthias-course).\n",
    "Retrieve your API key and copy this to the variable below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd55a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from cohere import CohereError\n",
    "apikey=\"\"\n",
    "co = cohere.Client(apikey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2bce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell\"\n",
    "co.generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d416e2",
   "metadata": {},
   "source": [
    "## In-context learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0ea65",
   "metadata": {},
   "source": [
    "Few-shot in-context learning consists in providing some examples, structured in a certain way (\"prompt engineering\") and leverage the fact that LLM are good in pattern-matching. Two examples below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d8fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The capital of Germany is Berlin.\n",
    "The capital of France is Paris.\n",
    "The capital of Albania is\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "co.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"text: I loved that movie.\n",
    "sentiment: positive\n",
    "text: It was a disgusting food.\n",
    "sentiment: negative\n",
    "text: I felt welcomed as if I was a king.\n",
    "sentiment: positive\n",
    "text: The service was truly exceptionally good.\n",
    "sentiment:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502330f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "co.generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2673cc5",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a530b42",
   "metadata": {},
   "source": [
    "For evaluation, using F1 measure you will need to compute the entities which are in common between the gold label and the proposed ones.\n",
    "As many of the posts do not have any entity we will be focusing on **recall** as evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db780f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one(gold, proposed):\n",
    "    \"\"\" returns (number of common hits, length of retrieved entities, length of gold entities)\n",
    "    params:\n",
    "        - gold: set of gold entities\n",
    "        - proposed: set of retrieved entities\n",
    "    \"\"\"\n",
    "    common = len(gold.intersection(proposed))\n",
    "    return common, len(proposed), len(gold)\n",
    "\n",
    "def precision(common, prop, gold):\n",
    "    prec = 1\n",
    "    if prop != 0: prec   = common/prop\n",
    "    return prec\n",
    "\n",
    "def recall(common, prop, gold):\n",
    "    recall = 1\n",
    "    if gold != 0: recall = common/gold \n",
    "    return recall\n",
    "\n",
    "def f1(common, prop, gold):\n",
    "    prec = precision(common, prop, gold)\n",
    "    rec  = recall(common, prop, gold)\n",
    "    f1 = 0\n",
    "    if prec+rec>0:\n",
    "        f1 = 2*prec*rec / (prec+rec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6c535",
   "metadata": {},
   "source": [
    "# The Exercise\n",
    "You will need to create a prompt (play around with some alternatives), and obtain a result for each text in the test set. The final average of recalls should be > 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75209d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test = dataset['test']\n",
    "recalls = []\n",
    "\n",
    "for idx,datapoint in enumerate(test):\n",
    "    thisprompt = ... #  generate an appropiate prompt \n",
    "    thisprompt += ... # concante something out of datapoint\n",
    " \n",
    "    try:\n",
    "        c = co.generate(thisprompt)\n",
    "        answer = c.generations[0].text\n",
    "    except CohereError as e:\n",
    "        continue\n",
    "    \n",
    "    proposed = .... # extract the entities from this text\n",
    "\n",
    "    gold = set([e.value.lower() for e in entities(t)])\n",
    "    r_score = recall(*evaluate_one(gold,proposed))\n",
    "    recalls.append(r_score)\n",
    "\n",
    "print np.mean(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7fcdb",
   "metadata": {},
   "source": [
    "# (Bonus) \n",
    "Now, let's observe how the model behave given different configurations. In particular : we can ask the following questions : How does the perfomance of the model depend on the number of training examples used for prompting ? On the training samples ? On the prompting methods ? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbc842",
   "metadata": {},
   "source": [
    "### Exp1 : Varying the train data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf461ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_n_samples(train, test, n_sampling):\n",
    "    all_recalls = []\n",
    "    n_train_samples = []\n",
    "    for n in range(1, n_sampling+1):\n",
    "        recalls = [] # recalls for exp with n train examples\n",
    "        \n",
    "        train_subset = ... # subset of training data of size n\n",
    "        prompt = ... # Prompt with n train examples\n",
    "        ... # use what was done before to obtain the results using the current prompt for n train samples\n",
    "        mean_recall = ... \n",
    "        \n",
    "        recalls.append(mean_recall)\n",
    "        n_train_samples.append(n)\n",
    "    \n",
    "    return n_train_samples, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sampling = ...\n",
    "n_train_samples, exp1_recalls = run_with_n_samples(train, test, n_sampling)\n",
    "df = pd.DataFrame(data={\"n_train_samples\": n_train_samples, \"recall\": recalls})\n",
    "sns.lineplot(data=df, x=\"n_train_samples\", y=\"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe3c6e",
   "metadata": {},
   "source": [
    "### Exp2 : Varying the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fcecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_varying_samples(train, test, n_sampling):\n",
    "    recalls = []\n",
    "    for n in range(n_sampling):\n",
    "        train_sample = ... # random training sample of size n\n",
    "        ... # use what was done before to obtain the results using the train subset\n",
    "        mean_recall = ... \n",
    "        \n",
    "        recalls.append(mean_recall)\n",
    "        \n",
    "    return recalls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sampling = ...\n",
    "exp2_recalls = run_with_varying_samples(train, test, n_sampling)\n",
    "print(np.mean(exp2_recalls), np.std(exp2_recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf990cf9",
   "metadata": {},
   "source": [
    "### Exp3: Try different prompting ways and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b123c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
